# Limitations and ethical considerations

## Tecnical challenges

- **Anti-scraping measures**: many platforms implement rate limits, CAPTCHAs, dynamic rendering, and bot detection. Changes in the DOM and APIs frequently break automation. It is necessary to design data collection processes that are resilient, log selector versions, and maintain alternatives such as official APIs or web archives.

- **Information overload and noise**: the volume of data and duplication make it difficult to identify relevant signals. Effective handling requires applying filters, deduplication, sampling, and prioritization aligned with Intelligence Requirements (IR).

- **Synthetic content (deepfakes/generated text)**: images, audio, and text generated by AI are increasingly common. Analysts must verify origin (when available, using metadata or provenance standards such as C2PA), perform basic forensic checks (visual, spatial, or temporal inconsistencies), and corroborate information using methods like the Rule of Two or temporal/geospatial triangulation.

- **Volatility and disappearance of historical data**: retention policies, content deletion, and ephemeral formats (stories, streams) reduce traceability. Captures should be timestamped, evidence hashed, archives (Wayback or institutional archives) consulted, and chain-of-custody procedures followed.

## **Analyst OPSEC (operations security)**

- **Identity separation**: use isolated accounts and research environments (VMs, depersonalized browsers), controlling metadata leaks (time, language, fingerprinting).

- **Attack surface**: prevent malvertising, malicious downloads, and tracking; employ blocklists, sandboxing, and up-to-date security tools.

- **Minimal interaction**: observe without participating; avoid likes, comments, or messages that could alter the environment or reveal the analystâ€™s presence.